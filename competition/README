
---------------------------------------------------------------------
        Benchmark Package for the GADGET-3 Code (2012 version)

	(Note: GADGET-4 will be coming soon...)
---------------------------------------------------------------------

This brief document contains:

  (1) Compilation instructions

  (2) A description for setting up the intial conditions for the
      test problem(s).

  (3) Instructions for running the test problems with Gadget3, and
      for extracting performance metrics.



(1) Compilation instructions.
-----------------------------

The GADGET-3 code, as well as the included initial conditions code
N-GENIC, are ANSI-C codes which use the MPI-1.1 libarary for
parallelization. (For some code parts, GADGET-3 can also in addition
use either Pthreads or OpenMP for a hybrid MPI/shared-memory
parallelization. Full support for this in all relevant code parts will
be included only in GADGET-4.)

The GADGET-3 code requires two open source C-libaries. These are

   - FFTW, available at http://www.fftw.org 
     (Note: Here, still the most recent version of FFTW-2 needs
     to be used, i.e. version 2.1.5)

   - GSL, availavle at http://www.gnu.org/software/gsl 

For compiling FFTW-2, one needs to pass "--enable-mpi" to the
configure-script. I also recommend to compile the code both in a
double and in a single-precision version. This can be accomplished by
building and installing the package twice, once with
"--enable-type-prefix", and the second time with "--enable-type-prefix
--enable-float".

Once these two libaries are available, the Makefile of the two codes
GADGET-3 and N-GENIC may need some adjustments to the target
systems. This should however be reasonably straightforward. (Look for
the "SYSTYPE" examples for various architectures, and add a block
corresponding to your system. The desired target system is then
selected either through the file Makefile.systype, or through the
environment variable SYSTYPE. See the beginning of the Makefile for
more explanations.)

GADGET has a (large) number of compile-time flags that are normally
defined in the file "Config.sh". However, it is possible to override
this by specifying a config file in the make command, e.g. in the form:

   make CONFIG=Config-Large.sh

This will build the executable "P-Gadget3" with the settings
appropriate for the "large" problem size defined in this benchmark. It
is also possible to override the executable name, e.g. through
 
   make CONFIG=Config-Large.sh  EXEC=LargeGadget3

There are three different config-files prepared for the present
benchmark set, and included in this distribution. They correspond to
"small", "medium" and "large" problem sizes, and should require no
changes (except if there is a compilation issue with the FFTW-2
library in case it has not been compiled for dual precision use, in
which case you may have to active NO_TYPE_PREDIX). It is suggested
that the executables for the three problem sizes are generated as
follows:

  make clean;  make CONFIG=Config-Small.sh  EXEC=Gadget3-Small

  make clean;  make CONFIG=Config-Medium.sh EXEC=Gadget3-Medium

  make clean;  make CONFIG=Config-Large.sh  EXEC=Gadget3-Large

(Note that the only real difference between the different
configurations lies in the settings of PMGRID, which determines the
mesh sized used in the long-range force calculation. It will be 
possible to run each of the executables on the 'wrong' problem size, 
but then the performance will in general be quite bad because the
adopted mesh size is then sub-optimum.)


(2) Generating initial conditions for the test runs
---------------------------------------------------

The simulation code GADGET-3 evolves cosmological density fields,
either only with dark matter, or with dark matter and gas. Any such
calculation requires suitable initial conditions, which can be easily
several hundred GBytes in size for large problems. Therefore, a
parallel initial conditions code is provided as part of this benchmark
package as well. It is called N-GenIC.

The IC code is run by passing a parameterfile as argument, e.g. as
follows:

       mpirun -np 16 ./N-GenIC  ics.param

The output directory is specified in the parameterfile "ics.param" and
may be changed if desired. The result will be distributed into several
files, one for each processor. The number of processors used to
generate the initial conditions is irrelevant for the results, and for
running the actual simulation, a different processor number can be
used.

There are setups for three problem sizes provided with this benchmark
package, a "small" one, a "medium" one, and a "large" one (64 times
bigger in size). Benchmark measurements should be provided for all
three problem sizes, although the small one is best viewed as a quick
test for the setup. All three problems are for a cosmological
simulation with both dark matter and gas particles, and the benchmark
code evolves the first three timesteps in full, and then terminates
automatically. As all relevant codes parts are exercised in this test,
the resulting timings give a good estimate for the speed of full-scale
production runs with Gadget3.

The predefined ICs have the following characteristics:

  Small:  2 x 128^3 particles,  total peak memory need  ~2.2 GB

  Medium: 2 x 512^3 particles,  total peak memory need  ~140 GB

  Large:  2 x 2048^3 particles, total peak memory need  ~8.8 TB   

Parameterfiles for creating the three sets of initial conditions are given in the 
three files

     ics_small.param

     ics_medium.param

     ics_large.param

contained in the N-GenIC subdirectory. There is also an example
job-script file for running this on a typical cluster (in this case
CURIE), but running the IC-generating jobs on another architecture
should be straightforward.

NOTE: On big endian machines (such as Bluegene/P), the code needs to
be fed a different input file for setting up the unperturbed particle
load. You can achieve this by copying "grid_big_endian.dat" to
"grid.dat". (For reversing this, copy "grid_little_endian.dat" to
"grid.dat").

The default output directory specified for the ICs code is "../ICs"
relativ to the N-GenIC code directory.  This can be changed if needed
by modifying "OutputDir" in the parameterfiles. For the small problem,
the produced files by N-GenIC consist of a series of files named
"ICs_128.*", for the medium one they are "ICs_512.*", and for the
large one "ICs_2048.*". There will be as many files as processors used
to run N-GenIC - this number must be large enough to fit the FFTs into
memory, as well as to hold the IC data itself. For the large problem
size, about 1-2 TB of memory is needed (less than for the actual
production run with Gadget3). Note that a given set of ICs can be run
with Gadget using a different number of MPI tasks than employed for
generating the ICs with N-GenIC. So on a machine with 2 GB/core, the ICs
could be generated with something like:

   mpirun -np 8    ./N-GenIC  ics_small.param  

   mpirun -np 128  ./N-GenIC  ics_medium.param  

   mpirun -np 1024 ./N-GenIC  ics_large.param  


(3) Executing the simulation code
---------------------------------

Ideally, the runtime of the GADGET-3 code is tested for each problem
size with a few different numbers of MPI tasks, which then constitutes
a small strong scaling test. In addition, runs of the different problem
sizes can be considered as a weak scaling test if runs with constant
particle load are compared.

We suggest the following set of runs: (Note that level 0 runs may 
require slightly more than 2 GB of RAM per core, and may not be 
possible on every system for this reason.)


  Run-Name  Problem-Size  MPI-Tasks    Execution time    
  ---------------------------------------------------
    S1      small               1              
    S2	    small               2
    S3      small               4              
    S4	    small               8              
    S5	    small              16
  
    M0      medium             32          
    M1      medium             64          
    M2      medium            128          
    M3      medium            256                  
    M4      medium            512          
    M5      medium           1024                  

    L0      large            2048        
    L1      large            4096         
    L2      large            8192          
    L3      large           16384          
   --------------------------------------------------------

   (S) small  = 2 x 128^3  =       4.194.304  particles
   (M) medium = 2 x 512^3  =     268.435.456  particles
   (L) large  = 2 x 2048^3 =  17.179.869.184  particles


where the last column should be filled in with the total excution time
after three timesteps (see below). A sequence such as S1/M1/L1 then 
respresents a weak scaling test, one such as M0/M1/M2/M3/M4/M5 a strong 
scaling test. (We note that for L2 and L3, the PM part of Gadget's 
calculation is not expected to scale any more due the chosen FFTW size 
in this test, 4096^3, due to FFTW's slab based decomposition, while the
tree-based part should still be able to scale. Strong scaling is hence 
expected to take a hit beyond ~8192 cores. In principle, this could be 
improved either with a larger PMGRID, by using a combination of fewer 
MPI tasks and threads, or by resorting to Gadget-4.)


Like the initial conditions code, the simulation code GADGET-3 is
started with a parameterfile as an argument, e.g. as follows:

       mpirun -np 128  ./Gadget3-Medium  param-medium.txt
       
The simulation code has been set up for these tests such that it will 
terminate itself automatically after 3 full steps have been completed, 
which gives a runtime of order less than 10 minutes. (The I/O cost 
can be somewhat more sizable for the big run though.)

Appropriate parameterfiles for all three different problem sizes are
included, as "param-small.txt", "param-medium.txt", and
"param-large.txt". In these parameterfiles, the setting for
"OutputDir" may be changed, which directs where the performance metric
output will go. Also, the parameter "MaxMemSize", which gives the
maximum allowed memory consumption of the code per MPI Task (in MB),
could be changed in case the memory for a test run turns out to be
insufficient for smaller partition sizes. The code will accurately
keep track of its memory consumption and not overstep this
bound. (Giving the code more room than it needs will normally not
improve performance.)

The execution times in differents parts of the code during these 3
steps represent the result of the bechmark. The code itself measures
several internal performance metrics by timing certain parts of the
code (using MPI_Wallclock). The output of the code (which includes
these performance metrics) is written to a separate directory for each
run. The pathname of this directory needs to specified as the
"OutputDir" variable in the parameterfile passed to the code.

The primary result of a benchmark run is contained in the file
"cpu.txt" which is put into the output directory. Actually, the
benchmark code will create a subfiled "run_XX" automatically in the
output directory for a run with XX MPI tasks such that different runs
are conveniently separated.  This file should be kept for each run for
a detailed evaluation as it contains a break-down of the wallclock
times required for the most important parts of the code, as
labelled. For each entry, the first number gives the cumulative time
spent for this part in seconds, while the second number gives the
relative contribution of this part to the total time in percent. There
are three primary calculations in the code that consume the
cycles. These are the short-range gravity done by a hierarchical
tree-algorithm ("treegrav"), the long-range gravity with a FFT-based
particle-mesh algorithm ("pmgrav"), and the smoothed particle
hydrodynamics ("sph"). The domain decomposition also consumes a bit of
time ("domain"), the other parts should be largely negligible.
The reported measurements exclude start-up time for 
initializing the code and for reading in the ICs.

For the purpose of evaluating the overall performance, the time
reported for "total" after Step 3 should be reported in the above
table. For example, the relevant lines close to the end of cpu.txt may
read like this:

Step 3, Time: 0.0204637, MPI-Tasks: 128  
total             130.56  100.0%
treegrav           75.66   58.0%
   treebuild        1.69    1.3%
   treeupdate       0.00    0.0%
   treewalk        68.41   52.4%
   treecomm         0.39    0.3%
   treeimbal        3.88    3.0%
pmgrav             12.71    9.7%
sph                30.76   23.6%
   density         11.60    8.9%
   denscomm         0.23    0.2%
   densimbal        0.50    0.4%
   hydrofrc        16.25   12.4%
   hydcomm          0.31    0.2%
   hydmisc          0.67    0.5%
   hydnetwork       0.00    0.0%
   hydimbal         0.66    0.5%
   hmaxupdate       0.13    0.1%
domain              6.03    4.6%
potential           0.00    0.0%
predict             0.00    0.0%
kicks               0.95    0.7%
i/o                 0.00    0.0%
peano               1.17    0.9%
sfrcool             0.00    0.0%
blackholes          0.00    0.0%
fof/subfind         0.00    0.0%
smoothing           0.00    0.0%
hotngbs             0.00    0.0%
weights_hot         0.00    0.0%
enrich_hot          0.00    0.0%
weights_cold        0.00    0.0%
enrich_cold         0.00    0.0%
cs_misc             0.00    0.0%
misc                3.26    2.5%


------>>  Here, the time to be reported is 130.56.

           
---------------------------------------------------------------------
In case of problems with these benchmark programs, please direct your
questions to  Volker Springel <volker.springel@uni-hd.de> 
---------------------------------------------------------------------

